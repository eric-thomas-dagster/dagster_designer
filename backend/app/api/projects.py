"""API endpoints for project management."""

import sys
import json
import subprocess
from pathlib import Path

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field

from ..models.project import (
    Project,
    ProjectCreate,
    ProjectUpdate,
    ProjectListResponse,
    ProjectSummaryListResponse,
)
from ..services.project_service import project_service
from ..services.asset_introspection_service import asset_introspection_service

router = APIRouter(prefix="/projects", tags=["projects"])


class CloneRepoRequest(BaseModel):
    """Request to clone a git repository."""
    git_repo: str
    git_branch: str = "main"


class CloneRepoResponse(BaseModel):
    """Response from cloning a repository."""
    project: Project
    repo_name: str


class ProjectImportRequest(BaseModel):
    """Request to import an existing Dagster project."""
    path: str = Field(..., description="Absolute path to existing Dagster project directory")


@router.post("", response_model=Project, status_code=201)
async def create_project(project_create: ProjectCreate, background_tasks: BackgroundTasks):
    """Create a new pipeline project.

    NOTE: Project is returned immediately! Dependency installation and asset generation
    happen in the background. Use the /dependency-status endpoint to check progress.
    """
    project = project_service.create_project(project_create)

    # Return project immediately - dependencies will be installed in background
    print(f"✅ Project created: {project.name}. Starting background dependency installation...")

    # Start dependency installation in background
    background_tasks.add_task(project_service.install_dependencies_async, project)

    return project


@router.get("/{project_id}/dependency-status")
async def get_dependency_status(project_id: str):
    """Get the dependency installation status for a project.

    Returns:
        {
            "status": "idle" | "installing" | "success" | "error",
            "error": str | null
        }
    """
    status = project_service.get_dependency_status(project_id)
    return status


@router.post("/import", response_model=Project, status_code=201)
async def import_project(request: ProjectImportRequest):
    """Import an existing Dagster project from disk.

    This will:
    1. Validate the path is a valid Dagster project
    2. Copy the project to the projects directory
    3. Create a project record in the backend

    NOTE: Asset introspection is NOT performed during import to avoid timeouts.
    The frontend should call the regenerate-assets endpoint after import to discover assets.
    """
    try:
        project = project_service.import_project(request.path)

        # Return project immediately - assets will be generated by frontend calling regenerate-assets
        print(f"✅ Project imported: {project.name}. Frontend should call regenerate-assets to discover assets.")

        return project
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to import project: {str(e)}")


@router.get("/summary", response_model=ProjectSummaryListResponse)
async def list_projects_summary():
    """List all projects with minimal metadata (optimized for large lists).

    This endpoint is much faster than /projects for displaying project lists
    because it only loads essential metadata without graphs, components, etc.
    """
    projects = project_service.list_projects_summary()
    return ProjectSummaryListResponse(projects=projects, total=len(projects))


@router.get("", response_model=ProjectListResponse)
async def list_projects():
    """List all projects with full data."""
    projects = project_service.list_projects()
    return ProjectListResponse(projects=projects, total=len(projects))


@router.get("/{project_id}", response_model=Project)
async def get_project(project_id: str):
    """Get a project by ID."""
    project = project_service.get_project(project_id)

    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    return project


@router.put("/{project_id}", response_model=Project)
async def update_project(project_id: str, project_update: ProjectUpdate):
    """Update a project."""
    import sys

    print(f"\n[API PUT /projects/{project_id}] Received update request", flush=True)
    print(f"[API] ProjectUpdate fields set: {project_update.model_dump(exclude_unset=True).keys()}", flush=True)
    sys.stdout.flush()

    project = project_service.update_project(project_id, project_update)

    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    print(f"[API PUT /projects/{project_id}] Update completed successfully", flush=True)
    sys.stdout.flush()
    return project


@router.delete("/{project_id}", status_code=204)
async def delete_project(project_id: str):
    """Delete a project."""
    success = project_service.delete_project(project_id)

    if not success:
        raise HTTPException(status_code=404, detail="Project not found")

    return None


@router.delete("/{project_id}/component-instances/{component_id}", response_model=Project)
async def delete_component_instance(project_id: str, component_id: str):
    """Delete a component instance (asset created from a component).

    This removes the component instance directory (/defs/{component_id}/)
    and regenerates assets to reflect the change.

    Args:
        project_id: The project ID
        component_id: The component instance ID (asset name)
    """
    from pathlib import Path
    import shutil

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    project_dir = project_service._get_project_dir(project)
    directory_name = project.directory_name

    # Try both flat and src layouts
    flat_defs_dir = project_dir / directory_name / "defs" / component_id
    src_defs_dir = project_dir / "src" / directory_name / "defs" / component_id

    component_instance_dir = None
    if flat_defs_dir.exists():
        component_instance_dir = flat_defs_dir
    elif src_defs_dir.exists():
        component_instance_dir = src_defs_dir

    if not component_instance_dir:
        raise HTTPException(
            status_code=404,
            detail=f"Component instance '{component_id}' not found"
        )

    try:
        # Delete the component instance directory
        shutil.rmtree(component_instance_dir)
        print(f"[Delete Component Instance] Deleted {component_instance_dir}")

        # Clear the asset introspection cache to force fresh introspection
        asset_introspection_service.clear_cache(project.id)
        print(f"[Delete Component Instance] Cleared asset cache for project {project.id}")

        # Regenerate assets to reflect the change
        # Get assets from dg list defs (using async version for non-blocking)
        asset_nodes, asset_edges = await asset_introspection_service.get_assets_for_project_async(project, recalculate_layout=False)
        print(f"[Delete Component Instance] Got {len(asset_nodes)} nodes, {len(asset_edges)} edges from introspection")

        # Update project graph with assets (preserving any non-asset nodes)
        non_asset_nodes = [n for n in project.graph.nodes if n.node_kind != "asset"]
        project.graph.nodes = non_asset_nodes + asset_nodes

        # Merge introspected edges with custom lineage edges
        from ..models.graph import GraphEdge

        # Create a map of edge IDs for quick lookup
        edge_map = {edge.id: edge for edge in asset_edges}

        # Process custom lineage edges
        for custom_lineage in project.custom_lineage:
            edge_id = f"{custom_lineage.source}_to_{custom_lineage.target}"

            if edge_id in edge_map:
                # Edge already exists from introspection, just mark it as custom
                edge_map[edge_id].is_custom = True
            else:
                # Edge doesn't exist, create a new custom edge
                edge_map[edge_id] = GraphEdge(
                    id=edge_id,
                    source=custom_lineage.source,
                    target=custom_lineage.target,
                    is_custom=True
                )

        # Convert edge map back to list
        project.graph.edges = list(edge_map.values())

        # Save updated project
        updated_project = project_service.update_project(project_id, ProjectUpdate(graph=project.graph))

        if not updated_project:
            raise HTTPException(
                status_code=500,
                detail="Failed to save project after deletion"
            )

        return updated_project

    except Exception as e:
        print(f"[Delete Component Instance] Error: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"Failed to delete component instance: {str(e)}"
        )


@router.post("/{project_id}/clone-repo", response_model=CloneRepoResponse)
async def clone_repo(project_id: str, request: CloneRepoRequest):
    """Clone a git repository for a project."""
    result = project_service.clone_repo_for_project(
        project_id,
        request.git_repo,
        request.git_branch
    )

    if not result:
        raise HTTPException(status_code=404, detail="Project not found")

    project, repo_name = result
    return CloneRepoResponse(project=project, repo_name=repo_name)


@router.post("/{project_id}/regenerate-assets", response_model=Project)
async def regenerate_assets(project_id: str, recalculate_layout: bool = False):
    """Regenerate assets for a project by running dg list defs.

    This introspects the Dagster components and updates the project graph
    with the actual assets that will be generated.

    Args:
        project_id: The project ID
        recalculate_layout: If True, recalculates all node positions instead of preserving existing ones
    """
    import sys
    print(f"\n[regenerate_assets] ====== START ======", flush=True)
    print(f"[regenerate_assets] project_id: {project_id}, recalculate_layout: {recalculate_layout}", flush=True)
    sys.stdout.flush()

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    try:
        # Clear cache to ensure fresh introspection
        asset_introspection_service.clear_cache(project.id)
        print(f"[regenerate_assets] Cleared asset cache for project {project.id}", flush=True)

        # Get assets from dg list defs (using async version for non-blocking)
        asset_nodes, asset_edges = await asset_introspection_service.get_assets_for_project_async(project, recalculate_layout=recalculate_layout)
        print(f"[regenerate_assets] Got {len(asset_nodes)} nodes, {len(asset_edges)} edges from introspection", flush=True)
        sys.stdout.flush()

        # Update project graph with assets (preserving any non-asset nodes and partition configs)
        non_asset_nodes = [n for n in project.graph.nodes if n.node_kind != "asset"]

        # Create a map of existing partition configs by node ID
        existing_partition_configs = {}
        for node in project.graph.nodes:
            if node.data and node.data.get("partition_config"):
                existing_partition_configs[node.id] = node.data["partition_config"]

        # Preserve partition configs in regenerated asset nodes
        for node in asset_nodes:
            if node.id in existing_partition_configs:
                if not node.data:
                    node.data = {}
                node.data["partition_config"] = existing_partition_configs[node.id]
                print(f"[regenerate_assets] Preserved partition config for node: {node.id}", flush=True)

        project.graph.nodes = non_asset_nodes + asset_nodes

        # Merge introspected edges with custom lineage edges
        from ..models.graph import GraphEdge

        # Create a map of edge IDs for quick lookup
        edge_map = {edge.id: edge for edge in asset_edges}

        # Process custom lineage edges
        custom_edges_added = 0
        for custom_lineage in project.custom_lineage:
            edge_id = f"{custom_lineage.source}_to_{custom_lineage.target}"

            if edge_id in edge_map:
                # Edge already exists from introspection, just mark it as custom
                edge_map[edge_id].is_custom = True
                print(f"[regenerate_assets] Marked existing edge as custom: {edge_id}", flush=True)
            else:
                # Edge doesn't exist, create a new custom edge
                edge_map[edge_id] = GraphEdge(
                    id=edge_id,
                    source=custom_lineage.source,
                    target=custom_lineage.target,
                    is_custom=True
                )
                custom_edges_added += 1
                print(f"[regenerate_assets] Added new custom edge: {edge_id}", flush=True)

        # Convert edge map back to list
        project.graph.edges = list(edge_map.values())

        print(f"[regenerate_assets] Updated project.graph: {len(project.graph.nodes)} nodes, {len(asset_edges)} introspected edges + {custom_edges_added} new custom edges = {len(project.graph.edges)} total", flush=True)
        print(f"[regenerate_assets] Calling update_project to save...", flush=True)
        sys.stdout.flush()

        # Save updated project
        updated_project = project_service.update_project(project_id, ProjectUpdate(graph=project.graph))

        print(f"[regenerate_assets] update_project returned project with {len(updated_project.graph.nodes if updated_project else 0)} nodes", flush=True)
        print(f"[regenerate_assets] ====== END ======\n", flush=True)
        sys.stdout.flush()

        return updated_project if updated_project else project

    except Exception as e:
        print(f"[regenerate_assets] ERROR: {e}", flush=True)
        sys.stdout.flush()
        raise HTTPException(status_code=500, detail=f"Failed to regenerate assets: {str(e)}")


@router.delete("/{project_id}/regenerate-assets/cache")
async def clear_regenerate_cache(project_id: str):
    """Clear the asset introspection cache for a specific project.

    This forces the next regenerate-assets call to run dg list defs
    instead of using cached results.
    """
    from ..services.asset_introspection_service import _assets_cache

    if project_id in _assets_cache:
        del _assets_cache[project_id]
        print(f"[Cache] Cleared asset introspection cache for project {project_id}", flush=True)
        return {"message": f"Cache cleared for project {project_id}"}

    return {"message": f"No cache found for project {project_id}"}


@router.get("/{project_id}/assets")
async def get_project_assets(project_id: str):
    """Get all assets from the stored project graph (instant, no dg list defs required).

    This returns assets that are already stored in the project graph from the last
    regenerate-assets call. Perfect for populating dropdowns, job asset selection, etc.

    Args:
        project_id: Project ID

    Returns:
        List of assets with their keys, names, descriptions, and checks
    """
    project = project_service.get_project(project_id)

    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    # Extract assets from the stored graph
    assets = []
    for node in project.graph.nodes:
        if node.node_kind == "asset":
            asset_data = {
                "key": node.data.get("asset_key", ""),
                "name": node.data.get("name", ""),
                "description": node.data.get("description", ""),
                "kinds": node.data.get("kinds", []),
                "group_name": node.data.get("group_name", "default"),
                "owners": node.data.get("owners", []),
                "tags": node.data.get("tags", []),
                "is_executable": node.data.get("is_executable", True),
                "automation_condition": node.data.get("automation_condition"),
                "checks": node.data.get("checks", []),
                "source": node.data.get("source", ""),
            }
            assets.append(asset_data)

    return {
        "project_id": project_id,
        "assets": assets,
        "total": len(assets),
        "source": "stored_graph"  # Indicates this came from stored graph, not fresh introspection
    }


@router.get("/{project_id}/asset-checks")
async def get_project_asset_checks(project_id: str):
    """Get all asset checks from the stored project graph (instant, no dg list defs required).

    This returns asset checks that are already stored in the project graph from the last
    regenerate-assets call. Perfect for the automations tab asset checks list.

    Args:
        project_id: Project ID

    Returns:
        List of all asset checks across all assets
    """
    project = project_service.get_project(project_id)

    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    # Extract asset checks from all assets in the stored graph
    asset_checks = []
    for node in project.graph.nodes:
        if node.node_kind == "asset":
            checks = node.data.get("checks", [])
            for check in checks:
                # Add the asset_key to each check for context
                check_data = {
                    "name": check.get("name", ""),
                    "key": check.get("key", ""),
                    "description": check.get("description", ""),
                    "asset_key": node.data.get("asset_key", ""),
                    "source": check.get("source", ""),
                }
                asset_checks.append(check_data)

    return {
        "project_id": project_id,
        "asset_checks": asset_checks,
        "total": len(asset_checks),
        "source": "stored_graph"  # Indicates this came from stored graph, not fresh introspection
    }


@router.post("/{project_id}/discover-components", response_model=Project)
async def discover_components(project_id: str):
    """Discover components from YAML files in an existing project.

    This scans the project's defs directory for component YAML files and
    adds them to the project. Useful for imported projects or when components
    are manually added outside of the designer.
    """
    try:
        project = project_service.discover_components_for_project(project_id)

        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        return project

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to discover components: {str(e)}")


@router.post("/{project_id}/validate")
async def validate_project(project_id: str):
    """Validate a project by running dg list defs and checking for errors.

    This checks if the project's component definitions are valid and can be loaded by Dagster.
    """
    import subprocess
    from pathlib import Path

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    project_dir = project_service._get_project_dir(project)
    venv_dg = project_dir / ".venv" / "bin" / "dg"

    if not venv_dg.exists():
        # Check if dependencies are currently being installed
        from ..services.project_service import _dependency_status
        dep_status = _dependency_status.get(project_id, {})
        is_installing = dep_status.get('status') in ['installing', 'pending']

        if is_installing:
            # Dependencies are still being installed - return pending status instead of error
            return {
                "valid": None,  # null indicates "pending/unknown" status
                "pending": True,
                "message": "Dependencies are still being installed. Validation will be available once installation completes.",
                "details": None
            }
        else:
            # Dependencies are not being installed and venv doesn't exist - this is an error
            return {
                "valid": False,
                "error": f"Project virtual environment not found at {venv_dg}. Please recreate the project.",
                "details": None
            }

    # Check if asset introspection/generation is currently in progress
    # Don't validate while assets are being generated as it may fail or timeout
    from ..services.asset_introspection_service import asset_introspection_service
    if asset_introspection_service._is_running(project_id):
        return {
            "valid": None,
            "pending": True,
            "message": "Asset generation is in progress. Validation will be available once assets are generated.",
            "details": None
        }

    try:
        # Run dg list defs to validate the project
        result = subprocess.run(
            [str(venv_dg.absolute()), "list", "defs"],
            cwd=str(project_dir.absolute()),
            capture_output=True,
            text=True,
            timeout=180  # 180 seconds for massive dbt projects
        )

        # If return code is 0, project is valid (even if there are warnings in stderr)
        if result.returncode == 0:
            # Count assets from output (look for the Assets section)
            has_assets = "Assets" in result.stdout and "│" in result.stdout

            # Return full output (no truncation)
            stdout_full = result.stdout if result.stdout else "No output"

            return {
                "valid": True,
                "message": "Project validation successful! All component definitions are valid." +
                         (f"\n\nFound assets in the project." if has_assets else ""),
                "details": {
                    "stdout": stdout_full,
                    "warnings": result.stderr if result.stderr else None  # Show full warnings
                }
            }
        else:
            # Extract error message from stderr
            error_lines = result.stderr.split('\n')

            # Look for validation error
            validation_error = None
            for i, line in enumerate(error_lines):
                if "ValidationError" in line or "Error" in line:
                    # Get surrounding context
                    start = max(0, i - 2)
                    end = min(len(error_lines), i + 10)
                    validation_error = '\n'.join(error_lines[start:end])
                    break

            return {
                "valid": False,
                "error": "Project validation failed. Component definitions have errors.",
                "details": {
                    "stderr": result.stderr,  # Full error output
                    "validation_error": validation_error
                }
            }

    except subprocess.TimeoutExpired:
        return {
            "valid": False,
            "error": "Validation timed out after 30 seconds",
            "details": None
        }
    except Exception as e:
        return {
            "valid": False,
            "error": f"Validation error: {str(e)}",
            "details": None
        }


class MaterializeRequest(BaseModel):
    """Request to materialize assets."""
    asset_keys: list[str] | None = None  # If None, materialize all assets
    config: dict | None = None  # Run config (ops, resources, execution, loggers)
    tags: dict[str, str] | None = None  # Run tags
    partition: str | None = None  # Specific partition to materialize


class MaterializeResponse(BaseModel):
    """Response from materializing assets."""
    success: bool
    message: str
    stdout: str
    stderr: str


@router.post("/{project_id}/materialize", response_model=MaterializeResponse)
async def materialize_assets(project_id: str, request: MaterializeRequest):
    """Materialize assets using dg launch command.

    This executes the dg launch command to materialize one or more assets.
    If no asset_keys are provided, all assets will be materialized.
    """
    import subprocess
    from pathlib import Path

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    # Get project path - construct it the same way as during project creation
    project_name_sanitized = project.name.lower().replace(" ", "_").replace("-", "_")
    project_dir_name = f"project_{project_id.split('-')[0]}_{project_name_sanitized}"
    project_path = Path("./projects") / project_dir_name
    if not project_path.exists():
        raise HTTPException(status_code=404, detail=f"Project directory not found: {project_path}")

    try:
        # Build dg launch command using project's venv
        venv_python = project_path / ".venv" / "bin" / "python"
        venv_dg = project_path / ".venv" / "bin" / "dg"

        if not venv_dg.exists():
            raise HTTPException(
                status_code=500,
                detail="Project virtual environment not found. Please ensure the project was created successfully."
            )

        # Build the command - use absolute path to dg binary
        cmd = [str(venv_dg.absolute()), "launch"]

        # Add asset selection if provided
        if request.asset_keys:
            cmd.extend(["--assets", ",".join(request.asset_keys)])
        else:
            # Materialize all assets - get all asset keys from project
            asset_keys = [node.data.get("asset_key", node.id) for node in project.graph.nodes if node.node_kind == "asset"]
            if asset_keys:
                cmd.extend(["--assets", ",".join(asset_keys)])
            else:
                return MaterializeResponse(
                    success=False,
                    message="No assets found to materialize",
                    stdout="",
                    stderr=""
                )

        # Add partition if provided
        if request.partition:
            cmd.extend(["--partition", request.partition])

        # Add config if provided
        config_file_path = None
        if request.config:
            import yaml
            import tempfile
            # Write config to temporary YAML file
            config_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False, dir=str(project_path))
            config_file_path = config_file.name
            yaml.dump(request.config, config_file)
            config_file.close()
            cmd.extend(["--config", config_file_path])

        # Add tags if provided
        if request.tags:
            import json
            # Tags are passed as part of run config in Dagster
            # We'll need to merge them into the config
            tags_config = {"tags": request.tags}
            if config_file_path:
                # Merge tags into existing config
                with open(config_file_path, 'r') as f:
                    existing_config = yaml.safe_load(f) or {}
                existing_config.update(tags_config)
                with open(config_file_path, 'w') as f:
                    yaml.dump(existing_config, f)
            else:
                # Create config file just for tags
                import tempfile
                config_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False, dir=str(project_path))
                config_file_path = config_file.name
                yaml.dump(tags_config, config_file)
                config_file.close()
                cmd.extend(["--config", config_file_path])

        print(f"[materialize] Running command: {' '.join(cmd)}")
        print(f"[materialize] Working directory: {project_path}")

        # Set up environment to use project's venv (use absolute paths)
        import os
        project_path_abs = project_path.absolute()
        venv_path_abs = project_path_abs / ".venv"

        env = os.environ.copy()
        env['VIRTUAL_ENV'] = str(venv_path_abs)
        env['PATH'] = f"{venv_path_abs / 'bin'}:{env.get('PATH', '')}"
        # Remove any parent venv variables that might confuse dg
        env.pop('PYTHONHOME', None)

        print(f"[materialize] Using venv: {venv_path_abs}")

        # Run command
        result = subprocess.run(
            cmd,
            cwd=str(project_path),
            env=env,
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )

        print(f"[materialize] Return code: {result.returncode}")
        print(f"[materialize] stdout: {result.stdout[:500]}")  # First 500 chars
        print(f"[materialize] stderr: {result.stderr[:500]}")  # First 500 chars

        success = result.returncode == 0
        message = "Materialization completed successfully" if success else "Materialization failed"

        # Clean up temporary config file
        if config_file_path:
            try:
                import os
                os.unlink(config_file_path)
            except Exception as e:
                print(f"[materialize] Warning: Failed to delete temp config file: {e}")

        return MaterializeResponse(
            success=success,
            message=message,
            stdout=result.stdout,
            stderr=result.stderr
        )

    except subprocess.TimeoutExpired:
        # Clean up config file on timeout
        if config_file_path:
            try:
                import os
                os.unlink(config_file_path)
            except:
                pass
        raise HTTPException(status_code=408, detail="Materialization timed out after 5 minutes")
    except Exception as e:
        # Clean up config file on error
        if config_file_path:
            try:
                import os
                os.unlink(config_file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"Failed to materialize assets: {str(e)}")


class CustomLineageRequest(BaseModel):
    """Request to add or remove custom lineage."""
    source: str  # Source asset key
    target: str  # Target asset key


@router.post("/{project_id}/custom-lineage", response_model=Project)
async def add_custom_lineage(project_id: str, request: CustomLineageRequest):
    """Add a custom lineage edge between two assets.

    This creates a dependency that will be injected into the asset specs
    via map_asset_specs in definitions.py.
    """
    from ..models.project import CustomLineageEdge, ProjectUpdate
    import traceback

    try:
        project = project_service.get_project(project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        # Add the custom lineage edge if it doesn't already exist
        new_edge = CustomLineageEdge(source=request.source, target=request.target)

        # Check if edge already exists
        exists = any(
            edge.source == new_edge.source and edge.target == new_edge.target
            for edge in project.custom_lineage
        )

        if not exists:
            updated_lineage = project.custom_lineage + [new_edge]
            updated_project = project_service.update_project(
                project_id,
                ProjectUpdate(custom_lineage=updated_lineage)
            )

            # Write custom_lineage.json to project directory
            try:
                project_service._write_custom_lineage_file(updated_project)
            except Exception as write_err:
                print(f"⚠️  Warning: Failed to write custom_lineage.json: {write_err}", flush=True)
                # Continue anyway - the lineage is saved in the DB

            return updated_project

        return project
    except HTTPException:
        raise
    except Exception as e:
        print(f"❌ Error adding custom lineage: {e}", flush=True)
        print(traceback.format_exc(), flush=True)
        raise HTTPException(status_code=500, detail=f"Failed to add custom lineage: {str(e)}")


@router.delete("/{project_id}/custom-lineage", response_model=Project)
async def remove_custom_lineage(project_id: str, request: CustomLineageRequest):
    """Remove a custom lineage edge between two assets."""
    from ..models.project import ProjectUpdate
    import traceback

    try:
        project = project_service.get_project(project_id)
        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        # Remove the custom lineage edge
        updated_lineage = [
            edge for edge in project.custom_lineage
            if not (edge.source == request.source and edge.target == request.target)
        ]

        updated_project = project_service.update_project(
            project_id,
            ProjectUpdate(custom_lineage=updated_lineage)
        )

        # Update custom_lineage.json file
        try:
            project_service._write_custom_lineage_file(updated_project)
        except Exception as write_err:
            print(f"⚠️  Warning: Failed to write custom_lineage.json: {write_err}", flush=True)
            # Continue anyway - the lineage is saved in the DB

        return updated_project
    except HTTPException:
        raise
    except Exception as e:
        print(f"❌ Error removing custom lineage: {e}", flush=True)
        print(traceback.format_exc(), flush=True)
        raise HTTPException(status_code=500, detail=f"Failed to remove custom lineage: {str(e)}")


class YAMLExportResponse(BaseModel):
    """Response from YAML export."""
    yaml_content: str
    filename: str


class YAMLImportRequest(BaseModel):
    """Request to import YAML pipeline."""
    yaml_content: str


@router.get("/{project_id}/export-yaml", response_model=YAMLExportResponse)
async def export_project_yaml(project_id: str):
    """Export project components as multi-document YAML.

    This generates a single YAML file with all components separated by `---`.
    Includes:
    - All component definitions
    - DependencyGraphComponent with edge information
    - Component attributes and configuration

    The exported YAML can be version controlled and imported back later.
    """
    import yaml

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    # Generate YAML documents for each component
    yaml_docs = []

    for component in project.components:
        doc = {
            'type': component.component_type,
            'attributes': component.attributes
        }

        # Add optional fields if present
        if component.label and component.label != component.id:
            doc['label'] = component.label
        if component.description:
            doc['description'] = component.description
        if hasattr(component, 'translation') and component.translation:
            doc['translation'] = component.translation
        if hasattr(component, 'post_processing') and component.post_processing:
            doc['post_processing'] = component.post_processing

        yaml_docs.append(yaml.dump(doc, default_flow_style=False, sort_keys=False))

    # Combine with --- separators
    yaml_content = "\n---\n\n".join(yaml_docs)

    # Generate filename
    project_name = project.name.replace(" ", "_").lower()
    filename = f"{project_name}_pipeline.yaml"

    return YAMLExportResponse(
        yaml_content=yaml_content,
        filename=filename
    )


@router.post("/{project_id}/import-yaml", response_model=Project)
async def import_project_yaml(project_id: str, request: YAMLImportRequest):
    """Import pipeline components from multi-document YAML.

    This replaces the project's components with those defined in the YAML.
    The YAML should contain documents separated by `---`, where each document defines a component.

    After import:
    - Existing components are replaced
    - Assets are regenerated from the new components
    - Graph is updated with new nodes and edges

    Example YAML format:
    ```yaml
    type: dagster_component_templates.RestApiFetcherComponent
    attributes:
      asset_name: api_data
      api_url: https://api.example.com/data

    ---

    type: dagster_component_templates.DependencyGraphComponent
    attributes:
      edges:
        - source: api_data
          target: cleaned_data
    ```
    """
    import yaml

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    try:
        # Parse multi-document YAML
        documents = list(yaml.safe_load_all(request.yaml_content))

        if not documents:
            raise HTTPException(status_code=400, detail="No YAML documents found")

        # Clear existing components
        project.components = []

        # Create components from YAML documents
        from ..models.component import ComponentInstance
        import uuid

        for i, doc in enumerate(documents):
            if not isinstance(doc, dict):
                raise HTTPException(
                    status_code=400,
                    detail=f"Document {i+1} is not a valid YAML object"
                )

            if 'type' not in doc:
                raise HTTPException(
                    status_code=400,
                    detail=f"Document {i+1} is missing 'type' field"
                )

            # Create component instance
            component = ComponentInstance(
                id=str(uuid.uuid4()),
                component_type=doc['type'],
                label=doc.get('label', doc.get('attributes', {}).get('asset_name', f"component_{i+1}")),
                description=doc.get('description', ''),
                attributes=doc.get('attributes', {}),
                translation=doc.get('translation'),
                post_processing=doc.get('post_processing'),
                is_asset_factory=True  # Assume all imported components are asset factories
            )

            project.components.append(component)

        # Save updated project
        updated_project = project_service.update_project(
            project_id,
            ProjectUpdate(components=project.components)
        )

        # Regenerate assets from the imported components
        try:
            asset_nodes, asset_edges = await asset_introspection_service.get_assets_for_project_async(updated_project)

            # Update graph with regenerated assets
            non_asset_nodes = [n for n in updated_project.graph.nodes if n.node_kind != "asset"]
            updated_project.graph.nodes = non_asset_nodes + asset_nodes
            updated_project.graph.edges = asset_edges

            # Save again with updated graph
            updated_project = project_service.update_project(
                project_id,
                ProjectUpdate(graph=updated_project.graph)
            )
        except Exception as e:
            print(f"⚠️  Failed to regenerate assets after import: {e}")
            # Continue anyway - assets can be regenerated manually

        return updated_project

    except yaml.YAMLError as e:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid YAML: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to import YAML: {str(e)}"
        )


@router.get("/{project_id}/community-component/{component_id}")
async def get_community_component_config(project_id: str, component_id: str):
    """Get the configuration of an installed community component.

    Args:
        project_id: Project ID
        component_id: Component folder name (e.g., "duckdb_table_writer")

    Returns:
        Component configuration from defs.yaml
    """
    import yaml
    from pathlib import Path

    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    project_dir = project_service._get_project_dir(project)
    directory_name = project.directory_name

    # Try both flat and src layouts
    flat_defs_dir = project_dir / directory_name / "defs" / component_id
    src_defs_dir = project_dir / "src" / directory_name / "defs" / component_id

    defs_yaml_path = None
    if flat_defs_dir.exists():
        defs_yaml_path = flat_defs_dir / "defs.yaml"
    elif src_defs_dir.exists():
        defs_yaml_path = src_defs_dir / "defs.yaml"

    if not defs_yaml_path or not defs_yaml_path.exists():
        raise HTTPException(
            status_code=404,
            detail=f"Community component '{component_id}' not found or not configured"
        )

    try:
        with open(defs_yaml_path, 'r') as f:
            defs_data = yaml.safe_load(f)

        # Also try to get manifest data for icon and category
        component_type = defs_data.get('type', '')
        component_dir = defs_yaml_path.parent.parent.parent / "components" / component_id
        manifest_path = component_dir / "manifest.yaml"

        icon = "package"  # Default icon
        category = "unknown"
        display_name = component_id.replace('_', ' ').title()

        if manifest_path.exists():
            try:
                with open(manifest_path, 'r') as f:
                    manifest_data = yaml.safe_load(f)
                    icon = manifest_data.get('icon', icon)
                    category = manifest_data.get('category', category)
                    display_name = manifest_data.get('name', display_name)
            except Exception as e:
                print(f"Failed to read manifest for {component_id}: {e}", flush=True)

        return {
            "component_id": component_id,
            "component_type": component_type,
            "attributes": defs_data.get('attributes', {}),
            "icon": icon,
            "category": category,
            "display_name": display_name,
        }
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to read component configuration: {str(e)}"
        )


# ============================================================================
# Partition & Config Endpoints
# ============================================================================


class PartitionInfoResponse(BaseModel):
    """Response containing partition information for an asset."""
    asset_key: str
    is_partitioned: bool
    partitions_def: dict | None = None


class ConfigSchemaResponse(BaseModel):
    """Response containing config schema for an asset."""
    asset_key: str
    has_config: bool
    config_schema: dict | None = None
    default_config: dict | None = None


@router.get("/{project_id}/assets/{asset_key:path}/partitions", response_model=PartitionInfoResponse)
async def get_asset_partitions(project_id: str, asset_key: str):
    """Get partition definition for an asset.

    Args:
        project_id: The project ID
        asset_key: The asset key (e.g., "my_asset" or "prefix/my_asset")

    Returns:
        PartitionInfoResponse with partition definition info
    """
    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    project_dir = project_service._get_project_dir(project)
    if not project_dir.exists():
        raise HTTPException(status_code=404, detail="Project directory not found")

    # Get the project module name
    project_module = project.directory_name

    try:
        # Run the show_partitions.py script from backend directory
        # Add project src to PYTHONPATH so it can import the project module
        import os
        env = os.environ.copy()
        project_src_dir = project_dir / "src"
        if "PYTHONPATH" in env:
            env["PYTHONPATH"] = f"{project_src_dir}:{env['PYTHONPATH']}"
        else:
            env["PYTHONPATH"] = str(project_src_dir)

        result = subprocess.run(
            [
                sys.executable,
                "-m",
                "scripts.show_partitions",
                project_module,
                asset_key,
            ],
            cwd=Path.cwd(),  # Run from backend directory
            env=env,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            # Try to parse error from stderr
            try:
                error_data = json.loads(result.stdout or result.stderr)
                raise HTTPException(
                    status_code=400,
                    detail=error_data.get("error", "Failed to get partition info")
                )
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to get partition info: {result.stderr}"
                )

        # Parse the JSON output - extract JSON from stdout (may have warnings before it)
        stdout = result.stdout.strip()
        # Find the first line that starts with { (the JSON output)
        for line in stdout.split('\n'):
            if line.strip().startswith('{'):
                # Found the start of JSON, get everything from here
                json_start_idx = stdout.index(line)
                json_str = stdout[json_start_idx:]
                partition_info = json.loads(json_str)
                break
        else:
            raise HTTPException(
                status_code=500,
                detail=f"No JSON found in output: {stdout[:200]}"
            )

        return PartitionInfoResponse(**partition_info)

    except subprocess.TimeoutExpired:
        raise HTTPException(
            status_code=504,
            detail="Request timed out while fetching partition info"
        )
    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to parse partition info: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error fetching partition info: {str(e)}"
        )


@router.get("/{project_id}/assets/{asset_key:path}/config-schema", response_model=ConfigSchemaResponse)
async def get_asset_config_schema(project_id: str, asset_key: str):
    """Get config schema for an asset.

    Args:
        project_id: The project ID
        asset_key: The asset key (e.g., "my_asset" or "prefix/my_asset")

    Returns:
        ConfigSchemaResponse with config schema and defaults
    """
    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    project_dir = project_service._get_project_dir(project)
    if not project_dir.exists():
        raise HTTPException(status_code=404, detail="Project directory not found")

    # Get the project module name
    project_module = project.directory_name

    try:
        # Run the show_config.py script from backend directory
        # Add project src to PYTHONPATH so it can import the project module
        import os
        env = os.environ.copy()
        project_src_dir = project_dir / "src"
        if "PYTHONPATH" in env:
            env["PYTHONPATH"] = f"{project_src_dir}:{env['PYTHONPATH']}"
        else:
            env["PYTHONPATH"] = str(project_src_dir)

        result = subprocess.run(
            [
                sys.executable,
                "-m",
                "scripts.show_config",
                project_module,
                asset_key,
            ],
            cwd=Path.cwd(),  # Run from backend directory
            env=env,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode != 0:
            # Try to parse error from stderr
            try:
                error_data = json.loads(result.stdout or result.stderr)
                raise HTTPException(
                    status_code=400,
                    detail=error_data.get("error", "Failed to get config schema")
                )
            except json.JSONDecodeError:
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to get config schema: {result.stderr}"
                )

        # Parse the JSON output - extract JSON from stdout (may have warnings before it)
        stdout = result.stdout.strip()
        # Find the first line that starts with { (the JSON output)
        for line in stdout.split('\n'):
            if line.strip().startswith('{'):
                # Found the start of JSON, get everything from here
                json_start_idx = stdout.index(line)
                json_str = stdout[json_start_idx:]
                config_info = json.loads(json_str)
                break
        else:
            raise HTTPException(
                status_code=500,
                detail=f"No JSON found in output: {stdout[:200]}"
            )

        return ConfigSchemaResponse(**config_info)

    except subprocess.TimeoutExpired:
        raise HTTPException(
            status_code=504,
            detail="Request timed out while fetching config schema"
        )
    except json.JSONDecodeError as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to parse config schema: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error fetching config schema: {str(e)}"
        )


class BackfillRequest(BaseModel):
    """Request to launch a backfill for partitioned assets."""
    asset_keys: list[str]
    partition_selection: list[str] | None = Field(
        None,
        description="List of partition keys to backfill, or None for all partitions"
    )
    partition_range: dict[str, str] | None = Field(
        None,
        description="Partition range with 'start' and 'end' keys"
    )
    config: dict | None = None
    tags: dict[str, str] | None = None


class BackfillResponse(BaseModel):
    """Response from launching a backfill."""
    success: bool
    message: str
    stdout: str
    stderr: str


@router.post("/{project_id}/backfill", response_model=BackfillResponse)
async def launch_backfill(project_id: str, request: BackfillRequest):
    """Launch a backfill for partitioned assets.

    Args:
        project_id: The project ID
        request: Backfill configuration

    Returns:
        BackfillResponse with backfill status
    """
    project = project_service.get_project(project_id)
    if not project:
        raise HTTPException(status_code=404, detail="Project not found")

    project_dir = project_service._get_project_dir(project)
    if not project_dir.exists():
        raise HTTPException(status_code=404, detail="Project directory not found")

    # Build the dg asset backfill command
    cmd = [
        project_dir / ".venv" / "bin" / "dg",
        "asset",
        "backfill",
    ]

    # Add asset selection
    if request.asset_keys:
        for asset_key in request.asset_keys:
            cmd.extend(["--select", asset_key])

    # Add partition selection
    if request.partition_selection:
        # Convert list of partitions to comma-separated string
        cmd.extend(["--partitions", ",".join(request.partition_selection)])
    elif request.partition_range:
        # Use partition range
        start = request.partition_range.get("start")
        end = request.partition_range.get("end")
        if start and end:
            cmd.extend(["--partitions", f"[{start}...{end}]"])
        elif start:
            cmd.extend(["--from", start])

    # Handle config and tags
    config_file = None
    try:
        if request.config or request.tags:
            # Create temporary config file
            config_data = {}
            if request.config:
                config_data.update(request.config)
            if request.tags:
                if "tags" not in config_data:
                    config_data["tags"] = {}
                config_data["tags"].update(request.tags)

            import tempfile
            import yaml

            with tempfile.NamedTemporaryFile(
                mode="w",
                suffix=".yaml",
                dir=project_dir,
                delete=False
            ) as f:
                yaml.dump(config_data, f)
                config_file = Path(f.name)

            cmd.extend(["--config", str(config_file)])

        # Execute the backfill command
        result = subprocess.run(
            cmd,
            cwd=project_dir,
            capture_output=True,
            text=True,
            timeout=300,  # 5 minute timeout for backfills
        )

        success = result.returncode == 0
        message = "Backfill launched successfully" if success else "Backfill failed"

        return BackfillResponse(
            success=success,
            message=message,
            stdout=result.stdout,
            stderr=result.stderr,
        )

    except subprocess.TimeoutExpired:
        return BackfillResponse(
            success=False,
            message="Backfill command timed out",
            stdout="",
            stderr="Command exceeded 5 minute timeout",
        )
    except Exception as e:
        return BackfillResponse(
            success=False,
            message=f"Error launching backfill: {str(e)}",
            stdout="",
            stderr=str(e),
        )
    finally:
        # Clean up temporary config file
        if config_file and config_file.exists():
            try:
                config_file.unlink()
            except Exception:
                pass
